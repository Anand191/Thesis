\nonumchap{Abstract}
    %
	In recent years deep neural networks have become the dominant architectural paradigm in the field of machine learning and have even bested human beings at the complex board game Go \citep{Silver2016}. However despite their amazing feats deep neural networks are brittle in the sense that they find it difficult to cope with data different to the one the network has been trained on. Human beings on the other hand can one shot generalize to new data i.e. learn from a single sample. This can be attributed to the fact that human beings have the algebraic capacity \citep{marcus2003algebraic} to compose complex meaningful expression from known primitives in a systematic fashion.
	
	In this thesis I explore the concept of systematic compositionality and propose a novel mechanism called \lq Attentive Guidance\rq{} (AG) to bias attention based sequence to sequence (seq2seq) models towards compositional solutions. Using a dataset which exhibits functional nesting and hierarchical compositionality, I show that while a vanilla seq2seq model with attention resorts to finding spurious patterns in data, AG finds a compositional solution and is able to generalize to unseen cases. AG is subsequently tested on a rule based dataset which requires a model to infer the probabilistic production rules from the data distribution. Finally to test the pattern recognition and compositional skills of a learner equipped with AG I introduce a new dataset grounded in sub regular language hierarchy. Over the course of this thesis, attention is motivated as a key requirement for a compositional learner. AG introduces learning biases in a seq2seq model without any architectural overhead and paves the way for future research into integrating components from human cognition into the training process of deep neural networks.


