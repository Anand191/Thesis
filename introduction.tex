\chapter{Introduction}\label{Chpater:intro}
	%Motivation
	\nomenclature{RNN}{Recurrent Neural Network}
	\nomenclature{$\rho_f$}{density of particle}
	\nomenclature{$\mu_f$}{dynamic viscosity of fluid}
	\nomenclature{$\nu_f$}{kinematic viscosity of fluid}
	\nomenclature{$C_{p,f}$}{specific heat capacity of fluid}
	\nomenclature{$C_{p,p}$}{specific heat capacity of particle}
	\nomenclature{$k_f$}{thermal conductivity of fluid}
	
The concept of an artificial neuron has existed since 1940s \citep{McCulloch1943} but they have come to dominate the spectrum of artificial intelligence research since \cite{Krizhevsky2012} beat the previous state of the art on the ImageNet challenge \citep{Deng2009} using their deep convolution neural network (CNN; \citep{Lecun89}). Since then deep learning has revolutionized the field of artificial intelligence and become the new state of the art in areas such as object recognition \citep{He2015}, speech recognition \citep{Graves2013} and machine translation \citep{Sutskever2014}.

The biggest criticism however of deep learning is its overt dependence on voluminous training data which has led many researchers to argue that deep neural networks are only good at pattern recognition within their training distribution \citep{Marcus2018} and therefore conform to the basic tenet of statistical machine learning that train and test data should come from the same distribution \citep{Zadrozny:2004:LEC:1015330.1015425}. Deep neural networks, therefore, are still poor at to test data which, despite coming from the same \lq rule space{}\rq,\ doesn't follow the exact same distribution as training data. In contrast, human reasoning is governed by a rule based systematicity \citep{FODOR19883} which leads us to learn complex concepts or rich representations from finite set of primitives. Borrowing an example from \cite{Lake2016}, human beings can learn to distinguish a segway from a bicycle from just one example, while, for doing the same, a deep network might require hundreds of images of both classes. 

\cite{Lake2016} argues that one of the ways of learning rich concepts in a data efficient manner is to build compositional representations. The segway in the previous example, for instance, can be represented as two wheels, connected by a base on which a handlebar is mounted while, for a bicycle, the representation could be two wheels, connected by a chain-drive all of which is mounted on a frame consisting of a rod,a seat and handlebars. A model which already has learned the \lq concept\rq{} of a bicycle compositionally as described above, can re-use that knowledge to learn the representations of new parts, subparts and their relations in the case of a segway faster and in a more data efficient manner. The concept of Compositionality is central to this thesis and therefore warrants an in-depth look which is presented in the subsequent section.

\section{Motivation} \label{Chapter:motivation}
Humans exhibit algebraic compositionality in their thought and reasoning \citep{marcus2003algebraic}. I discuss the concept of compositionality and briefly review how current deep neural networks deal with it. 

%The concept of attention in human beings is then briefly touched upon, since a key model (section \ref{mtv:attn}) inspired  from this cognitive feature of human beings will be a recurring theme throughout this work. Furthermore attention also lies at the core of the major contribution of this thesis presented in chapter \ref{Chapter:proposals}.

\subsection{Systematic Compositionality} \label{systematic}
Compositionality is the principle of understanding a complex expression through the meaning and syntactic combination of its morphemes \citep{Pelletier1994}. This definition of compositionality does not directly imply dependence on context in which the expression appears or the intent of the speaker and therefore the compositional nature of natural language is an active area of debate among linguists and philosophers \citep{sep-compositionality}. Despite that compositionality is arguably a crucial part of natural language owing to the fact new meaningful complex expressions can be formed systematically by combining known words via. valid syntactic operations.

%One of the strongest arguments in the favour of compositionality in natural language is the presence of systematicity \citep{fodor1989psychosemantics}. 
%\textbf{Systematicity} - Provided [E1, E3] of same grammatical category and [E2,E4] of same grammatical category. If [E1,E2] can combine syntactically then so can [E1, E4], [E3, E2] and [E3,E4] and if one can understand ([E1,E2]) and ([E3, E4]) then they can also understand ([E1,E4]) and ([E3,E2]) provided that the is well formed.

One way to make progress in understanding and modeling compositionality is to focus on artificial languages, since they can be constructed to strictly follow the principles of systematic compositionality, leaving out the debated ingredients such as the influence of context and intentionality. In the next section, we present some artificial datasets which have been specifically created in accordance with the principle of compositionality.

%Aritifical languages (such as the ones presented in the next section) on the other hand can be constructed to strictly follow the principles of systematic compositionality.

\subsection{Compositionality and Deep Learning}
Deep neural networks have been shown to possess a modicum of compositional learning. \cite{LeCun2015} have argued that deep learning is adept at discovering hierarchical structures from data. For instance, in computer vision, deep neural networks learn primitive shapes (lines, circles etc.) in the shallower layer and, from that, sub-parts of an object in deeper layers \citep{Zeiler2014}. Although impressive, it can argued that this is an instance of hierarchical feature learning and it still requires thousands if not millions of samples (imagenet; \citep{Deng2009}).

In the recent years researchers have focused on creating new domains of learning which are inherently compositional and cannot be solved by mere pattern recognition. For instance the CLEVR dataset introduced by \cite{Johnson2017} tests the visual reasoning abilities of a system such as attribute identification, counting objects, attending to multiple objects and logical operations. 

%An argument in favour of compositional learning to solve the CLEVR task is presented by \cite{Hudson2018} as ``suppose you have been asked the following: `What is the yellow thing that is right of the small cube in front of the green object made of?'. How would you approach such a question?''. 

\cite{Lake2017} introduced the SCAN dataset which maps a string of commands to the corresponding string of actions in a completely compositional manner and is therefore a natural setting for seq2seq models (section \ref{background:s2s}). The commands consist of \textit{primitives} such as jump, walk, run etc.; \textit{direction modifiers} such as left, right, opposite etc.; \textit{counting modifiers} such as twice, thrice and \textit{conjuctions} and, after. This grammar generates a finite set of unambiguous commands which can be decoded if the meaning of the morphemes are well understood. The experiments on this dataset by the authors indicates that seq2seq models fails to extract the \textit{systematic} rules from the grammar which are required for generalization to test data which doesn't come from the same distribution as the train data but follows the same underlying rules. Seq2seq models generalize well (and in a data efficient manner) on novel samples from the same distribution on which they have been trained, but fail to generalize to samples which exhibit the same systematic rules as the training data but come from a different statistical distribution viz. longer/unseen compositions of commands. These results indicate that seq2seq models lack the algebraic capacity to compose complex expressions from simple morphemes by operating in a \lq rule space{}\rq\ and rather resort to pattern recognition mechanisms.

The inability of a vanilla seq2seq model to solve a compositional task was shown by \cite{Liska2018}. The authors introduced a new dataset consisting of atomic tables which map 3-bit strings bijectively to 3-bit strings and can these atomic tables can be applied sequentially to a given input to yield compositions that show functional hierarchy and nesting. This task should be fairly intuitive for a compositional system given sufficient memory to store all the atomic tables. In their experiments the authors concluded that only a small fraction of all trained models found a compositional solution.

%\subsection{Attention in Humans} \label{intro:attn}
%Attention is the aspect of human cognitive faculties whereby one can choose to focus selectively of a discrete aspect of information and therefore can also be viewed as selective allocation of brain's processing resources \citep{anderson2005cognitive}. The concept of attention has inspired the seminal work in sequence prediction \citep{Bahdanau2014} which I will review in detail in section \ref{mtv:attn}. Attention has also inspired work in the fields of object detection \citep{BaMK14}, visual question answering \citep{lu2016hierarchical} and image generation \citep{GregorDGW15}.

		
\section{Objectives}
	%
	Objectives

	
\section{Outline}
	%
	\textbf{Chapter \ref{Chapter:Theory}} presents the theoretical background. It consists of a brief overview of the neural network architectues used throughout this thesis. It also presents the concepts of attention and pondering which are central to the work presented in the chapter \ref{Chapter:proposals}. This chapter also briefly cover formal language theory which serves as the foundation for chapter \ref{Chapter:datasets}.
	
	\textbf{Chapter \ref{Chapter:proposals}} focuses on the major contribution of this thesis - attentive guidance. I explain the implementation of this method and then introduce the datasets used to test it. Comprehensive results and their discussion for each of the datasets have been presented in this chapter.
	
%	\textbf{Chapter \ref{Chapter:experiments}} outlines the experimental setup and performance metrics used for the lookup table and symbol rewriting tasks respectively.
	
%	\textbf{Chapter \ref{Chapter:results}} presents the results of baseline and attentive guidance models on lookup table and symbol rewriting tasks. A comprehensive analysis of these results has also been presented in this chapter.
	
	\textbf{Chapter \ref{Chapter:datasets}} presents a new dataset called Micro Tasks grounded in formal language theory, as a new setting for testing compositionality of attentive guidance. The experiments and results on this dataset for both baseline and attentive guidance follow the dataset description.
	
	\textbf{Chapter \ref{Chapter:conclusion}} summarizes the thesis and lists possible directions of future work.


				