\chapter{Introduction}
	%Motivation
	\nomenclature{RNN}{Recurrent Neural Network}
	\nomenclature{$\rho_f$}{density of particle}
	\nomenclature{$\mu_f$}{dynamic viscosity of fluid}
	\nomenclature{$\nu_f$}{kinematic viscosity of fluid}
	\nomenclature{$C_{p,f}$}{specific heat capacity of fluid}
	\nomenclature{$C_{p,p}$}{specific heat capacity of particle}
	\nomenclature{$k_f$}{thermal conductivity of fluid}
	
"Can Start with some quote"
How deep learning has taken the world by storm since Alexnet in 2012. Examples of some areas where it has reached human/super-human level performance.

However data hungry and thus poor at concept learning. Where has it failed?

The biggest criticism of deep learning is its overt dependence on voluminous data which has led many researchers to argue that deep networks are only good at finding pattern recognition in training distribution (give refs) and therefore conform to the basic tenet of statistical machine learning that train and test data should come from the same distribution(give refs). Deep neural networks therefore are still poor at zero shot generalization to test data which despite coming from the same \lq rule space{}\rq\ don't follow the exact same distribution as training data (find a good example here). Human reasoning on the other hand is governed by a rule based systematicity (give refs) which leads us to learn complex concepts from small samples which leads to zero shot generalization (example ).


\section{Motivation} \label{Chapter:motivation}
Humans exhibit algebraic compositionality in their thought and reasoning (ref). We discuss the concept of compositionality and breifly review how current deep networks deal with it. The review of these concepts is essential since they are my guiding principles for this work.

\subsection{Systematic Compositionality} \label{systematic}
Compositionality is the principle of understanding a complex expression through the meaning and syntactic combination of its morphemes. This definition of compositionality does not directly imply dependence on context in which the expression appears or the intent of the speaker and therefore the compositional nature of natural language is an active area of debate among linguists and philosophers \citep{sep-compositionality}. One of the strongest arguments in the favour of compositionality in natural language is the presence of systematicity (cite Fodor). 
%
\textbf{Systematicity}. Provided [E1, E3] of same grammatical category and [E2,E4] of same grammatical category. If [E1,E2] can combine syntactically then so can [E1, E4], [E3, E2] and [E3,E4] and if one can understand ([E1,E2]) and ([E3, E4]) then they can also understand ([E1,E4]) and ([E3,E2]) provided that the is well formed.

Aritifical languages (such as the ones we'll encounter in chapter \ref{Chapter:datasets}) on the other hand can be constructed to strictly follow the principles of systematic compositionality. Given human propensity for compositional solutions \citep{NIPS2016_6130} these datasets provide excellent test-beds for validating the existence of compositional skills in neural networks. 

\subsection{Compositionality in Seq2Seq Models}
\cite{Lake2017} introduced the SCAN dataset which maps a string of commands to the corresponding string of actions and is therefore a natural setting for seq2seq models. The commands consist of \textit{primitives} such as jump, walk, run etc.; \textit{direction modifiers} such as left, right, opposite etc.; \textit{counting modifiers} such as twice, thrice and \textit{conjuctions} and, after. This grammar generates a finite set of unambiguous commands which can be decoded if the meaning of the morphemes are well understood. The experiments on this dataset by the authors indicates that seq2seq models fails to extract the \textit{systematic} rules from the grammar which are required for generalization to test data which doesn't come from the same distribution as train data but follows the same underlying rules. Seq2seq models generalize well (and in a data efficient manner) on novel samples from the same distribution on which they have been trained, but fail to generalize to samples which exhibit the same systematic rules as the training data but come from a different statistical distribution viz. longer/unseen compositions of commands. These results indicate that seq2seq models lack the algebraic capacity to compose complex expressions from simple morphemes by operating in a \lq rule space{}\rq\ and rather resort to pattern recognition mechanisms.

The inability if a vanilla seq2seq model to solve a compositional task was shown by \cite{Liska2018}.

\section{Attention in Humans}
Not too sure about this section at the moment. Let it be for now. Attention from a cognitive neuroscience perspective. Attentional blink

		
\section{Objectives}
	%
	Objectives

	
\section{Outline}
	%
	Thesis outline

				