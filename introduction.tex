\chapter{Introduction}
	%Motivation
	\nomenclature{RNN}{Recurrent Neural Network}
	\nomenclature{$\rho_f$}{density of particle}
	\nomenclature{$\mu_f$}{dynamic viscosity of fluid}
	\nomenclature{$\nu_f$}{kinematic viscosity of fluid}
	\nomenclature{$C_{p,f}$}{specific heat capacity of fluid}
	\nomenclature{$C_{p,p}$}{specific heat capacity of particle}
	\nomenclature{$k_f$}{thermal conductivity of fluid}
	
The concept of an artificial neuron has existed since 1940s \citep{McCulloch1943} but they or more specifically the phrase \lq neural networks \rq{} has entered common parlance since \cite{Krizhevsky2012} beat the previous state of the art on the ImageNet challenge \citep{Deng2009} using their deep convolution neural network (CNN; \citep{Lecun89}). Since then deep learning has revolutionized the field of artificial intelligence and become the new state of the art in areas such as object recognition \citep{He2015}, speech recognition \citep{Graves2013} and machine translation \citep{Sutskever2014} to name a few.

The biggest criticism however of deep learning is its overt dependence on voluminous data which has led many researchers to argue that deep networks are only good at pattern recognition in training distribution \citep{Marcus2018} and therefore conform to the basic tenet of statistical machine learning that train and test data should come from the same distribution \citep{Zadrozny:2004:LEC:1015330.1015425}. Deep neural networks therefore are still poor at zero shot generalization to test data which despite coming from the same \lq rule space{}\rq\ doesn't follow the exact same distribution as training data (find a good example here). Human reasoning on the other hand is governed by a rule based systematicity \citep{FODOR19883} which leads us to learn complex concepts or rich representations from finite (but small) set of primitives. Borrowing an example from \cite{Lake2016}, human beings can learn to distinguish a segway from bicycle from just one example (one-shot learning) while for doing the same a deep network might require hundreds of images of both classes. 

\cite{Lake2016} argues that one of the ways of learning rich concepts in a data efficient manner is to build compositional representations. The segway in the previous example for instance can be represented as two wheels, connected by a base on which a handlebar is mounted while for a bicycle the representation could be two wheels, connected by a chain-drive all of which is mounted on a frame consisting of a rod,a seat and handlebars. The concept of Compositionality is central to this thesis and therefore warrants an in-depth look which is presented in the subsequent section.

\section{Motivation} \label{Chapter:motivation}
Humans exhibit algebraic compositionality in their thought and reasoning (ref). I discuss the concept of compositionality and briefly review how current deep networks deal with it. The concept of attention in human beings is then briefly touched upon, since a key model inspired (section \ref{mtv:attn}) from this cognitive feature of human beings will,be a recurring theme throughout this work. Furthermore attention also lies at the core of the major contribution of this thesis presented in chapter \ref{Chapter:proposals}.

\subsection{Systematic Compositionality} \label{systematic}
\textbf{Frege's Principle;} \citep{Pelletier1994} - Compositionality is the principle of understanding a complex expression through the meaning and syntactic combination of its morphemes. This definition of compositionality does not directly imply dependence on context in which the expression appears or the intent of the speaker and therefore the compositional nature of natural language is an active area of debate among linguists and philosophers \citep{sep-compositionality}. One of the strongest arguments in the favour of compositionality in natural language is the presence of systematicity \citep{fodor1989psychosemantics}. 

\textbf{Systematicity} - Provided [E1, E3] of same grammatical category and [E2,E4] of same grammatical category. If [E1,E2] can combine syntactically then so can [E1, E4], [E3, E2] and [E3,E4] and if one can understand ([E1,E2]) and ([E3, E4]) then they can also understand ([E1,E4]) and ([E3,E2]) provided that the is well formed.

Aritifical languages (such as the ones presented in the next section) on the other hand can be constructed to strictly follow the principles of systematic compositionality. Given human propensity for compositional solutions \citep{NIPS2016_6130} these datasets provide excellent test-beds for validating the existence of compositional skills in neural networks. 

\subsection{Compositionality and Deep Learning}
Deep neural networks have been shown to possess some modicum of compositional learning. \cite{LeCun2015} have argued that deep learning is adept at discovering hierarchical structures from data. For instance they learn primitive shapes (lines, circles etc.) in the shallower layer and from that sub-parts of an object in deeper layers \citep{Zeiler2014}. Although impressive it can argued that this is an instance of hierarchical feature learning and even this requires thousand if not millions of samples (imagenet; \citep{Deng2009}).

In the recent years researchers have focused on creating new domains of learning which are inherently compositional and cannot be solved by mere pattern recognition. For instance the CLEVR dataset introduced by \cite{Johnson2017} tests the visual reasoning abilities of a system such as attribute identification, counting multiple attention and logical operations. An argument in favour of compositional learning to solve the CLEVR task is presented by \cite{Hudson2018} as ``suppose you have been asked the following: `What is the yellow thing that is right of the small cube in front of the green object made of?'. How would you approach such a question?''. 

\cite{Lake2017} introduced the SCAN dataset which maps a string of commands to the corresponding string of actions and is therefore a natural setting for seq2seq models (section \ref{background:s2s}). The commands consist of \textit{primitives} such as jump, walk, run etc.; \textit{direction modifiers} such as left, right, opposite etc.; \textit{counting modifiers} such as twice, thrice and \textit{conjuctions} and, after. This grammar generates a finite set of unambiguous commands which can be decoded if the meaning of the morphemes are well understood. The experiments on this dataset by the authors indicates that seq2seq models fails to extract the \textit{systematic} rules from the grammar which are required for generalization to test data which doesn't come from the same distribution as train data but follows the same underlying rules. Seq2seq models generalize well (and in a data efficient manner) on novel samples from the same distribution on which they have been trained, but fail to generalize to samples which exhibit the same systematic rules as the training data but come from a different statistical distribution viz. longer/unseen compositions of commands. These results indicate that seq2seq models lack the algebraic capacity to compose complex expressions from simple morphemes by operating in a \lq rule space{}\rq\ and rather resort to pattern recognition mechanisms.

The inability if a vanilla seq2seq model to solve a compositional task (lookup-tables) was shown by \cite{Liska2018}. The dataset consists of atomic tables which map 3-bit strings bijectively to 3-bit strings and can these atomic tables can be applied sequentially to a given input to yield compositions that show functional hierarchy and nesting. This task should be fairly intuitive for a compositional system given sufficient memory to store all the atomic tables. In their experiments the authors concluded that only a small fraction of all trained models landed on a compositional solution.

\subsection{Attention in Humans} \label{intro:attn}
Attention is the aspect of human cognitive faculties whereby one can choose to focus selectively of a discrete aspect of information and therefore can also be viewed as selective allocation of brain's processing resources \citep{anderson2005cognitive}. The concept of attention has inspired the seminal work in sequence prediction \citep{Bahdanau2014} which I will review in detail in section \ref{mtv:attn}. Attention has also inspired work in the fields of object detection \citep{BaMK14}, visual question answering \citep{lu2016hierarchical} and image generation \citep{GregorDGW15}.

		
\section{Objectives}
	%
	Objectives

	
\section{Outline}
	%
	\textbf{Chapter \ref{Chapter:Theory}} presents the theoretical background. It consists of a brief overview of the neural network architectues used throughout this thesis. It also presents the concepts of attention and pondering which are central to the work presented in the chapter \ref{Chapter:proposals}. This chapter also briefly cover formal language theory which serves as the foundation for chapter \ref{Chapter:datasets}.
	
	\textbf{Chapter \ref{Chapter:proposals}} focuses on the major contribution of this thesis - attentive guidance. I motivate the approach by presenting pitfalls of the current state of the art models for this domain and present the inspiration for this method. The chapter also discusses two datasets which were used as the test bed for this new model and comparing it to the baseline model.  
	
	\textbf{Chapter \ref{Chapter:experiments}} outlines the experimental setup and performance metrics used for the lookup table and symbol rewriting tasks respectively.
	
	\textbf{Chapter \ref{Chapter:results}} presents the results of baseline and attentive guidance models on lookup table and symbol rewriting tasks. A comprehensive analysis of these results has also been presented in this chapter.
	
	\textbf{Chapter \ref{Chapter:datasets}} presents a new dataset called Micro Tasks grounded in formal language theory, as a new setting for testing compositionality of attentive guidance. The experiments and results on this dataset for both baseline and attentive guidance follow the dataset description.
	
	\textbf{Chapter \ref{Chapter:conclusion}} summarizes the thesis and lists possible directions of future work.


				